



import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
from diffusers.utils.export_utils import export_to_video
import torch, uuid
from pathlib import Path
import numpy as np
from PIL import Image


# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"GPU: {device_name}, VRAM: {vram_gb:.2f} GB")
    
else:
    vram_gb = 0
    print("‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω ‚Äî –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU")
# === –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ –ø–æ VRAM ===
def get_generation_settings(vram_gb: float):
    """–ü–æ–¥–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É."""
    if vram_gb < 6:
        mode = "fast"
        settings = dict(height=320, width=576, num_frames=16, num_inference_steps=20)
    elif vram_gb < 10:
        mode = "balanced"
        settings = dict(height=448, width=768, num_frames=24, num_inference_steps=25)
    else:
        mode = "quality"
        settings = dict(height=576, width=1024, num_frames=32, num_inference_steps=30)

    print(f"üß† –†–µ–∂–∏–º: {mode.upper()} | {settings['height']}x{settings['width']} | "
          f"–ö–∞–¥—Ä–æ–≤: {settings['num_frames']} | –®–∞–≥–æ–≤: {settings['num_inference_steps']}")
    return settings
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
app = FastAPI()

output_dir = Path("output")
output_dir.mkdir(exist_ok=True)

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å...")

try:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    pipev2_576w = DiffusionPipeline.from_pretrained(
        "cerspense/zeroscope_v2_576w",
        torch_dtype=dtype
    ).to(device)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
    try:
        pipev2_576w.enable_model_cpu_offload()
    except Exception as e:
        print(f"‚ö†Ô∏è CPU offload –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

    # –í–∫–ª—é—á–∞–µ–º xFormers, –µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç VRAM
    if torch.cuda.is_available() and vram_gb >= 6:
        try:
            pipev2_576w.enable_xformers_memory_efficient_attention()
            print("‚úÖ Memory-efficient attention –≤–∫–ª—é—á–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å xFormers: {e}")
    else:
        print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ VRAM ‚Äî xFormers –Ω–µ –≤–∫–ª—é—á–µ–Ω")

    print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

except Exception as exc:
    raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {exc}")


class VideoRequest(BaseModel):
    prompt: str


@app.post("/generate")
async def generate(req: VideoRequest):
    try:
        print(f"üé¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {req.prompt}")

        settings = get_generation_settings(vram_gb)

        start_time = time.time()  # —Å—Ç–∞—Ä—Ç —Ç–∞–π–º–µ—Ä–∞

        output = pipev2_576w(
            req.prompt,
            num_inference_steps=settings["num_inference_steps"],
            height=settings["height"],
            width=settings["width"],
            num_frames=settings["num_frames"]
        )

        if not hasattr(output, "frames") or output.frames is None:
            raise RuntimeError("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ (frames –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)")

        frames = output.frames
        processed_frames = []

        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–æ–≤ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ) ---
        if isinstance(frames, np.ndarray):
            if frames.ndim == 5 and frames.shape[0] == 1:
                frames = frames[0]
            if frames.ndim == 4 and frames.shape[-1] == 3:
                for arr in frames:
                    arr = np.clip(arr * 255, 0, 255).astype(np.uint8)
                    processed_frames.append(Image.fromarray(arr))
        elif isinstance(frames, torch.Tensor):
            frames = frames.squeeze(0).detach().cpu().numpy()
            for arr in frames:
                arr = np.clip(arr * 255, 0, 255).astype(np.uint8)
                processed_frames.append(Image.fromarray(arr))
        elif isinstance(frames, list):
            for f in frames:
                if isinstance(f, Image.Image):
                    processed_frames.append(f.convert("RGB"))
                elif isinstance(f, (np.ndarray, torch.Tensor)):
                    arr = f.detach().cpu().numpy() if isinstance(f, torch.Tensor) else f
                    arr = np.clip(arr * 255, 0, 255).astype(np.uint8)
                    processed_frames.append(Image.fromarray(arr))

        # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–¥–µ–æ ---
        filename = f"{uuid.uuid4().hex}.mp4"
        output_path = output_dir / filename
        export_to_video(processed_frames, str(output_path), fps=8)

        # --- –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –∏ FPS ---
        elapsed = time.time() - start_time
        fps = len(processed_frames) / elapsed if elapsed > 0 else 0
        print(f"‚è± –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {elapsed:.2f} —Å–µ–∫ | FPS: {fps:.2f}")

        return {"video_path": str(output_path), "generation_time_sec": elapsed, "fps": fps}

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"–ó–∞–ø—Ä–æ—Å –ø–æ—Å—Ç—É–ø–∏–ª, –Ω–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞—á–∞–ª–∞—Å—å: {str(e)}"
        )



#docker run --gpus all -p 8080:8080 -v ${PWD}:/app zeroscope-server
