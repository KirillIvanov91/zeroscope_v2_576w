




from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
from diffusers.utils.export_utils import export_to_video
import torch, uuid
from pathlib import Path

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"GPU: {device_name}, VRAM: {vram_gb:.2f} GB")
else:
    vram_gb = 0
    print("‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω ‚Äî –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
app = FastAPI()

output_dir = Path("output")
output_dir.mkdir(exist_ok=True)

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å...")

try:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    pipev2_576w = DiffusionPipeline.from_pretrained(
        "cerspense/zeroscope_v2_576w",
        torch_dtype=dtype
    ).to(device)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
    try:
        pipev2_576w.enable_model_cpu_offload()
    except Exception as e:
        print(f"‚ö†Ô∏è CPU offload –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

    # –í–∫–ª—é—á–∞–µ–º xFormers, –µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç VRAM
    if torch.cuda.is_available() and vram_gb >= 6:
        try:
            pipev2_576w.enable_xformers_memory_efficient_attention()
            print("‚úÖ Memory-efficient attention –≤–∫–ª—é—á–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å xFormers: {e}")
    else:
        print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ VRAM ‚Äî xFormers –Ω–µ –≤–∫–ª—é—á–µ–Ω")

    print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

except Exception as exc:
    raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {exc}")


class VideoRequest(BaseModel):
    prompt: str


@app.post("/generate")
async def generate(req: VideoRequest):
    try:
        print(f"üé¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {req.prompt}")

        num_frames = 24 if vram_gb >= 6 else 8

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ
        output = pipev2_576w(
            req.prompt,
            num_inference_steps=10,
            height=320,
            width=576,
            num_frames=num_frames
        )

        if not hasattr(output, "frames") or len(output.frames) == 0:
            raise RuntimeError("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ (frames –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)")

        print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ
        filename = f"{uuid.uuid4().hex}.mp4"
        output_path = output_dir / filename
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ: {filename}")

        export_to_video(output.frames, str(output_path), fps=8)

        print(f"‚úÖ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")

        return {"video_path": str(output_path)}

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"–ó–∞–ø—Ä–æ—Å –ø–æ—Å—Ç—É–ø–∏–ª, –Ω–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞—á–∞–ª–∞—Å—å: {str(e)}"
        )


@app.get("/")
async def read_root():
    return {"message": "–°–µ—Ä–≤–∏—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∑–∞–ø—É—â–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç /generate."}




#docker run --gpus all -p 8080:8080 -v ${PWD}:/app zeroscope-server
